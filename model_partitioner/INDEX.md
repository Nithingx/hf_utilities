# Model Partitioner - Complete Index

Welcome to the Vision-Language Model Partitioner toolkit! This index will help you navigate all the components.

## ğŸ“š Documentation

| File | Purpose | Audience |
|------|---------|----------|
| [README.md](README.md) | Complete documentation with all features and options | All users |
| [QUICKSTART.md](QUICKSTART.md) | 5-minute getting started guide | New users |
| [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) | Technical implementation details | Developers |
| [INDEX.md](INDEX.md) | This file - navigation guide | All users |

## ğŸ”§ Core Implementation Files

### Main Orchestrator
- **`model_partitioner_v2.py`** (447 lines)
  - Main entry point for all operations
  - Implements all 5 run modes
  - Command-line interface
  - Performance tracking

### Pipeline Components
- **`vision_pipeline.py`** (184 lines)
  - Vision model inference (PyTorch, ONNX)
  - VitisAI EP support (ready)
  - Image preprocessing
  - Feature extraction

- **`language_pipeline.py`** (199 lines)
  - Language model inference (PyTorch, SafeTensors)
  - AWQ support (ready)
  - Text generation
  - Token management

### Conversion Utilities
- **`onnx_converter.py`** (276 lines)
  - PyTorch to ONNX conversion
  - Model validation
  - Optimization tools
  - Quantization support

### Legacy/Original
- **`model_partitioner.py`** (original version)
  - First implementation
  - Preserved for reference
  - Feature-complete with basic modes

## ğŸ¯ Run Modes

### Mode 1: Original Model
**Command:** `python model_partitioner_v2.py --mode original --image demo.jpg`

**What it does:**
- Runs unmodified VL model
- Provides baseline metrics
- No splitting or conversion

**Use case:** Baseline performance measurement

---

### Mode 2: Split Native
**Command:** `python model_partitioner_v2.py --mode split_native --image demo.jpg`

**What it does:**
- Splits vision â†’ PyTorch (.pt)
- Splits language â†’ SafeTensors (.safetensors)
- Runs with separated pipeline

**Use case:** Native format separation, debugging

---

### Mode 3: Convert ONNX
**Command:** `python model_partitioner_v2.py --mode convert_onnx --image demo.jpg`

**What it does:**
- Exports vision â†’ ONNX
- Keeps language â†’ SafeTensors
- Validates conversion

**Use case:** ONNX export, optimization preparation

---

### Mode 4: Run ONNX
**Command:** `python model_partitioner_v2.py --mode run_onnx --image demo.jpg`

**What it does:**
- Uses ONNX vision model
- Uses SafeTensor language model
- Runs end-to-end inference

**Use case:** Production deployment, optimized inference

---

### Mode 5: Save Standalone
**Command:** `python model_partitioner_v2.py --mode save_standalone --image demo.jpg`

**What it does:**
- Creates standalone directories
- Copies models with inference scripts
- Enables independent testing

**Use case:** Component debugging, independent deployment

---

### Mode 6: All
**Command:** `python model_partitioner_v2.py --mode all --image demo.jpg`

**What it does:**
- Runs all modes sequentially
- Compares performance
- Generates comprehensive report

**Use case:** Complete testing, performance comparison

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Your First Test
```bash
python model_partitioner_v2.py --mode original --image demo.jpg
```

### 3. Run All Modes
```bash
python run_all_examples.py --image demo.jpg
```

### 4. Check Results
```bash
# Results saved in split_models/ directory
ls -la split_models/
```

## ğŸ“ File Organization

```
model_partitioner/
â”‚
â”œâ”€â”€ ğŸ“˜ Documentation
â”‚   â”œâ”€â”€ README.md                      # Full documentation
â”‚   â”œâ”€â”€ QUICKSTART.md                 # Getting started
â”‚   â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md     # Technical details
â”‚   â””â”€â”€ INDEX.md                      # This file
â”‚
â”œâ”€â”€ ğŸ”§ Core Code
â”‚   â”œâ”€â”€ model_partitioner_v2.py       # Main orchestrator â­
â”‚   â”œâ”€â”€ vision_pipeline.py             # Vision inference
â”‚   â”œâ”€â”€ language_pipeline.py           # Language inference
â”‚   â”œâ”€â”€ onnx_converter.py             # ONNX conversion
â”‚   â””â”€â”€ model_partitioner.py          # Original version
â”‚
â”œâ”€â”€ ğŸ“ Examples
â”‚   â”œâ”€â”€ run_all_examples.py           # Example runner
â”‚   â””â”€â”€ requirements.txt              # Dependencies
â”‚
â””â”€â”€ ğŸ“¦ Generated (runtime)
    â””â”€â”€ split_models/
        â”œâ”€â”€ vision_model/              # .pt weights
        â”œâ”€â”€ language_model/            # .safetensors weights
        â”œâ”€â”€ onnx_model/               # ONNX models
        â”œâ”€â”€ standalone/               # Standalone scripts
        â””â”€â”€ model_config.json         # Metadata
```

## ğŸ¯ Common Use Cases

### Use Case 1: Quick Test
```bash
python model_partitioner_v2.py --mode original --image demo.jpg
```

### Use Case 2: Production Deployment
```bash
# Convert to optimized formats
python model_partitioner_v2.py --mode convert_onnx --image demo.jpg

# Run with optimized formats
python model_partitioner_v2.py --mode run_onnx --image demo.jpg
```

### Use Case 3: Debug Components
```bash
# Create standalone scripts
python model_partitioner_v2.py --mode save_standalone --image demo.jpg

# Test vision independently
cd split_models/standalone/vision
python vision_inference.py --image ../../../demo.jpg

# Test language independently
cd ../language
python language_inference.py --text "Your prompt"
```

### Use Case 4: Performance Comparison
```bash
# Run all modes and compare
python run_all_examples.py --image demo.jpg

# Check results
cat example_results_*.json
```

### Use Case 5: Memory-Constrained Environment
```bash
# Use quantization
python model_partitioner_v2.py --mode original --quantize --image demo.jpg
```

## ğŸ” Finding What You Need

### "I want to understand the project"
â†’ Start with [README.md](README.md)

### "I want to get started quickly"
â†’ Follow [QUICKSTART.md](QUICKSTART.md)

### "I want to understand the implementation"
â†’ Read [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)

### "I want to modify the vision pipeline"
â†’ Edit `vision_pipeline.py`

### "I want to modify the language pipeline"
â†’ Edit `language_pipeline.py`

### "I want to add ONNX optimizations"
â†’ Edit `onnx_converter.py`

### "I want to add new modes"
â†’ Edit `model_partitioner_v2.py`

### "I want to integrate VitisAI"
â†’ Update `vision_pipeline.py:_get_onnx_providers()`

### "I want to integrate AWQ"
â†’ Update `language_pipeline.py:load_awq_model()`

## ğŸ“Š Output Files Reference

### After Running Any Mode:

**Configuration:**
- `split_models/model_config.json` - Model metadata

**Vision Models:**
- `split_models/vision_model/vision_model.pt` - PyTorch format
- `split_models/onnx_model/vision_model.onnx` - ONNX format

**Language Models:**
- `split_models/language_model/language_model.safetensors` - SafeTensors format

**Standalone:**
- `split_models/standalone/vision/` - Vision-only inference
- `split_models/standalone/language/` - Language-only inference

**Results:**
- `example_results_*.json` - Performance comparison (from run_all_examples.py)

## ğŸ“ Learning Path

### Beginner
1. Read [QUICKSTART.md](QUICKSTART.md)
2. Run `--mode original`
3. Run `--mode split_native`
4. Explore output files

### Intermediate
1. Read [README.md](README.md) sections
2. Try all modes
3. Compare performance
4. Modify parameters

### Advanced
1. Read [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)
2. Understand pipeline architecture
3. Modify pipeline code
4. Integrate VitisAI/AWQ

## ğŸ”§ Command-Line Reference

### Basic Options
```bash
--mode {original|split_native|convert_onnx|run_onnx|save_standalone|all}
--image PATH           # Input image
--text TEXT            # Text prompt
--device {auto|cuda|cpu}
--max-tokens N         # Generation length
```

### Advanced Options
```bash
--model-id MODEL       # Different HF model
--quantize            # Enable 4-bit quantization
--output-dir DIR      # Output directory
```

## ğŸ› Troubleshooting Guide

### Problem: CUDA out of memory
**Solution:** Use `--quantize` or `--device cpu`

### Problem: Image not found
**Solution:** Provide valid image with `--image path/to/image.jpg`

### Problem: ONNX conversion fails
**Solution:** Ensure onnx and onnxruntime are installed

### Problem: Module not found
**Solution:** Run `pip install -r requirements.txt`

### Problem: Standalone scripts don't work
**Solution:** Run `--mode save_standalone` first

## ğŸ“ Getting Help

1. Check this INDEX for navigation
2. Read relevant documentation file
3. Run example scripts
4. Check error messages carefully
5. Verify dependencies installed

## ğŸ¯ Next Steps

### Ready to Start?
1. Install dependencies: `pip install -r requirements.txt`
2. Follow [QUICKSTART.md](QUICKSTART.md)
3. Run your first example

### Ready to Deploy?
1. Read [README.md](README.md) deployment section
2. Convert models with `--mode convert_onnx`
3. Test with `--mode run_onnx`
4. Use standalone scripts for production

### Ready to Extend?
1. Read [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)
2. Understand pipeline architecture
3. Fork and modify
4. Integrate your optimizations

## ğŸŒŸ Key Features Highlight

âœ… 5 complete run modes  
âœ… Modular pipeline architecture  
âœ… ONNX conversion with validation  
âœ… SafeTensors support  
âœ… Performance tracking  
âœ… Standalone deployment  
âœ… VitisAI infrastructure ready  
âœ… AWQ infrastructure ready  
âœ… Comprehensive documentation  
âœ… Example scripts included  

## ğŸ“ Version Info

- **Current Version:** 2.0
- **Python:** 3.8+
- **PyTorch:** 2.0+
- **Transformers:** 4.35+

---

**Happy Model Partitioning! ğŸš€**

For questions or issues, refer to the specific documentation files linked above.

